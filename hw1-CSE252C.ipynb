{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE252C: Homework 1\n",
    "## Computing Resources\n",
    "Please read the README file of this repository for the instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Submission instructions:**\n",
    "(a) Submit the notebook and its PDF version on Gradescope.\n",
    "(b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.\n",
    "(c) Correctly select pages for each answer on Gradescope to allow proper grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try SFM using the original implementation from $\\mathtt{libviso2}$. We will test on a dataset containing 300 images from one sequence of the KITTI dataset with ground-truth camera poses and camera calibration information. \n",
    "\n",
    "Run the SFM algorithm using the following script. You are required to report two error metrics. The error metric for rotation is defined as the mean of Frobenius norm of the difference between the ground-truth rotation matrix and predicted rotation matrix. The error metric for translation is defined as mean of the L2 distance. Both errors will be printed on the screen as you run the code.  **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# change your base path\n",
    "os.chdir('./pyviso/') # './'\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "import viso2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import time\n",
    "\n",
    "def errorMetric(RPred, RGt, TPred, TGt):\n",
    "    diffRot = (RPred - RGt)\n",
    "    diffTrans = (TPred - TGt)\n",
    "    errorRot = np.sqrt(np.sum(np.multiply(diffRot.reshape(-1), diffRot.reshape(-1))))\n",
    "    errorTrans = np.sqrt(np.sum(np.multiply(diffTrans.reshape(-1), diffTrans.reshape(-1))))\n",
    "\n",
    "    return errorRot, errorTrans\n",
    "\n",
    "if_vis = True # set to True to do the visualization per frame; the images will be saved at '.vis/'. Turn off if you just want the camera poses and errors\n",
    "if_on_screen = False # if True the visualization per frame is going to be displayed realtime on screen; if False there will be no display, but in both options the images will be saved\n",
    "\n",
    "# parameter settings (for an example, please download\n",
    "# dataset_path = '../dataset'\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM' # On the ``ieng6.ucsd.edu`` server\n",
    "img_dir      = os.path.join(dataset_path, 'sequences/00/image_0')\n",
    "gt_dir       = os.path.join(dataset_path, 'poses/00.txt')\n",
    "calibFile    = os.path.join(dataset_path, 'sequences/00/calib.txt')\n",
    "border       = 50;\n",
    "gap          = 15;\n",
    "\n",
    "# Load the camera calibration information\n",
    "with open(calibFile) as fid:\n",
    "    calibLines = fid.readlines()\n",
    "    calibLines = [calibLine.strip() for calibLine in calibLines]\n",
    "\n",
    "calibInfo = [float(calibStr) for calibStr in calibLines[0].split(' ')[1:]]\n",
    "# param = {'f': calibInfo[0], 'cu': calibInfo[2], 'cv': calibInfo[6]}\n",
    "\n",
    "# Load the ground-truth depth and rotation\n",
    "with open(gt_dir) as fid:\n",
    "    gtTr = [[float(TrStr) for TrStr in line.strip().split(' ')] for line in fid.readlines()]\n",
    "gtTr = np.asarray(gtTr).reshape(-1, 3, 4)\n",
    "\n",
    "# param['height'] = 1.6\n",
    "# param['pitch']  = -0.08\n",
    "# param['match'] = {'pre_step_size': 64}\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "epi = 1e-8\n",
    "\n",
    "# init visual odometry\n",
    "params = viso2.Mono_parameters()\n",
    "params.calib.f = calibInfo[0]\n",
    "params.calib.cu = calibInfo[2]\n",
    "params.calib.cv = calibInfo[6]\n",
    "params.height = 1.6\n",
    "params.pitch = -0.08\n",
    "\n",
    "\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "\n",
    "# init transformation matrix array\n",
    "Tr_total = []\n",
    "Tr_total_np = []\n",
    "Tr_total.append(viso2.Matrix_eye(4))\n",
    "Tr_total_np.append(np.eye(4))\n",
    "\n",
    "# init viso module\n",
    "visoMono = viso2.VisualOdometryMono(params)\n",
    "\n",
    "if if_vis:\n",
    "    save_path = 'vis'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # create figure\n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.axis('off')\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.set_xticks(np.arange(-100, 100, step=10))\n",
    "    ax2.set_yticks(np.arange(-500, 500, step=10))\n",
    "    ax2.axis('equal')\n",
    "    ax2.grid()\n",
    "    if if_on_screen:\n",
    "        plt.ion()\n",
    "    else:\n",
    "        plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all frames do\n",
    "if_replace = False\n",
    "errorTransSum = 0\n",
    "errorRotSum = 0\n",
    "errorRot_list = []\n",
    "errorTrans_list =[]\n",
    "\n",
    "for frame in range(first_frame, last_frame):\n",
    "    # 1-based index\n",
    "    k = frame-first_frame+1\n",
    "\n",
    "    # read current images\n",
    "    I = imread(os.path.join(img_dir, '%06d.png'%frame))\n",
    "    assert(len(I.shape) == 2) # should be grayscale\n",
    "\n",
    "    # compute egomotion\n",
    "    process_result = visoMono.process_frame(I, if_replace)\n",
    "    Tr = visoMono.getMotion()\n",
    "    matrixer = viso2.Matrix(Tr)\n",
    "    Tr_np = np.zeros((4, 4))\n",
    "    Tr.toNumpy(Tr_np) # so awkward...\n",
    "\n",
    "    # accumulate egomotion, starting with second frame\n",
    "    if k > 1:\n",
    "        if process_result is False:\n",
    "            if_replace = True\n",
    "            Tr_total.append(Tr_total[-1])\n",
    "            Tr_total_np.append(Tr_total_np[-1])\n",
    "        else:\n",
    "            if_replace = False\n",
    "            Tr_total.append(Tr_total[-1] * viso2.Matrix_inv(Tr))\n",
    "            Tr_total_np.append(Tr_total_np[-1] @ np.linalg.inv(Tr_np)) # should be the same\n",
    "            print(Tr_total_np[-1])\n",
    "\n",
    "    # output statistics\n",
    "    num_matches = visoMono.getNumberOfMatches()\n",
    "    num_inliers = visoMono.getNumberOfInliers()\n",
    "    matches = visoMono.getMatches()\n",
    "    matches_np = np.empty([4, matches.size()])\n",
    "\n",
    "    for i,m in enumerate(matches):\n",
    "        matches_np[:, i] = (m.u1p, m.v1p, m.u1c, m.v1c)\n",
    "\n",
    "    if if_vis:\n",
    "        # update image\n",
    "        ax1.clear()\n",
    "        ax1.imshow(I, cmap='gray', vmin=0, vmax=255)\n",
    "        if num_matches != 0:\n",
    "            for n in range(num_matches):\n",
    "                ax1.plot([matches_np[0, n], matches_np[2, n]], [matches_np[1, n], matches_np[3, n]])\n",
    "        ax1.set_title('Frame %d'%frame)\n",
    "\n",
    "        # update trajectory\n",
    "        if k > 1:\n",
    "            ax2.plot([Tr_total_np[k-2][0, 3], Tr_total_np[k-1][0, 3]], \\\n",
    "                [Tr_total_np[k-2][2, 3], Tr_total_np[k-1][2, 3]], 'b.-', linewidth=1)\n",
    "            ax2.plot([gtTr[k-2][0, 3], gtTr[k-1][0, 3]], \\\n",
    "                [gtTr[k-2][2, 3], gtTr[k-1][2, 3]], 'r.-', linewidth=1)\n",
    "        ax2.set_title('Blue: estimated trajectory; Red: ground truth trejectory')\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "    # Compute rotation\n",
    "    Rpred_p = Tr_total_np[k-2][0:3, 0:3]\n",
    "    Rpred_c = Tr_total_np[k-1][0:3, 0:3]\n",
    "    Rpred = Rpred_c.transpose() @ Rpred_p\n",
    "    Rgt_p = np.squeeze(gtTr[k-2, 0:3, 0:3])\n",
    "    Rgt_c = np.squeeze(gtTr[k-1, 0:3, 0:3])\n",
    "    Rgt = Rgt_c.transpose() @ Rgt_p\n",
    "    # Compute translation\n",
    "    Tpred_p = Tr_total_np[k-2][0:3, 3:4]\n",
    "    Tpred_c = Tr_total_np[k-1][0:3, 3:4]\n",
    "    Tpred = Tpred_c - Tpred_p\n",
    "    Tgt_p = gtTr[k-2, 0:3, 3:4]\n",
    "    Tgt_c = gtTr[k-1, 0:3, 3:4]\n",
    "    Tgt = Tgt_c - Tgt_p\n",
    "    # Compute errors\n",
    "    errorRot, errorTrans = errorMetric(Rpred, Rgt, Tpred, Tgt)\n",
    "    errorRotSum = errorRotSum + errorRot\n",
    "    errorTransSum = errorTransSum + errorTrans\n",
    "    # errorRot_list.append(errorRot)\n",
    "    # errorTrans_list.append(errorTrans)\n",
    "    print('Mean Error Rotation: %.5f'%(errorRotSum / (k-1+epi)))\n",
    "    print('Mean Error Translation: %.5f'%(errorTransSum / (k-1+epi)))\n",
    "\n",
    "\n",
    "\n",
    "    print('== [Result] Frame: %d, Matches %d, Inliers: %.2f'%(frame, num_matches, 100*num_inliers/(num_matches+1e-8)))\n",
    "\n",
    "    if if_vis:\n",
    "        # input('Paused; Press Enter to continue') # Option 1: Manually pause and resume\n",
    "        if if_on_screen:\n",
    "            plt.pause(0.1) # Or Option 2: enable to this to auto pause for a while after daring to enable animation in case of a delay in drawing\n",
    "        vis_path = os.path.join(save_path, 'frame%03d.jpg'%frame)\n",
    "        fig.savefig(vis_path)\n",
    "        print('Saved at %s'%vis_path)\n",
    "        \n",
    "        if frame % 50 == 0 or frame == last_frame-1:\n",
    "            plt.figure(figsize=(10, 15))\n",
    "            plt.imshow(plt.imread(vis_path))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# input('Press Enter to exit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then answer the questions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In $\\mathtt{libviso2}$, the feature points are \"bucketed\" ($\\mathtt{libviso2/src/matcher.cpp: Line 285 - 326}$), which means in a certain area of region, the number of detected keypoint pairs should be within certain bounds. Why?  **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We have run SFM on a single camera, which means the scale of translation is unknown. However, as you may have observed, the predicted trajectory is still somehow similar to the ground-truth trajectory. How does $\\mathtt{libviso2}$ handle this ambiguity ($\\mathtt{viso\\_mono.cpp: Line 175}$)?  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Briefly explain the RANSAC algorithm used in $\\mathtt{libviso2}$ ($\\mathtt{viso\\_mono.cpp: Line 91 - 105}$).  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Using SIFT [4] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second task, you are required to use keypoints and feature descriptors from SIFT for SFM. The SIFT implementation can be found in directory $\\mathtt{SIFT}$. \n",
    "\n",
    "(A) Go to $\\mathtt{SIFT}$ directory and run $\\mathtt{runSIFT.py}$ (e.g. `python runSIFT.py --input /datasets/cse152-252-sp20-pubc/dataset_SfM/sequences/00/image_0/`). You will save the detected keypoints and feature descriptors under the directory $\\mathtt{SIFT}$. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable should be a $130 \\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $130$-dimensional feature vector, the first two dimensions are the location of the keypoints (column number first and then row number) on the image plane and the last $128$ dimensions are the feature descriptor. \n",
    "\n",
    "(B) Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'SIFT'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Does SIFT yield higher accuracy than the original $\\mathtt{libviso2}$? Why or why not? If not, can you suggest one possible way to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain how SIFT achieves invariance to \n",
    "       a. illumination\n",
    "       b. rotation\n",
    "       c. scale\n",
    " **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Using SuperPoint[1] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are required to use keypoints and feature descriptors from SuperPoint for SFM. The code for the trained model of this method can be found from the $\\mathtt{SuperPoint}$.\n",
    "1. Go to $\\mathtt{SuperPoint}$ directory and run $\\mathtt{demo\\_superpoint.py}$. The detected keypoints and feature descriptors are under the directory $\\mathtt{SuperPoint}$. The file format is similar to the SIFT case. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable is a $258\\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $258$-dimensional feature vector, the first two dimensions are the locations of the keypoint (column number first and then row number) on the image plane and the last $256$ dimensions represent the feature descriptor. \n",
    "2. Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'SuperPoint'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Does SuperPoint yield higher accuracy than the original $\\mathtt{libviso2}$? If so, why? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain briefly how the following issues are being handled in SuperPoint: **(5 points)**\n",
    "       a. Obtaining ground truth for keypoints.\n",
    "       b. Cheaply obtaining accurate ground truth matches, as compared to LIDAR in UCN or SFM in LIFT.\n",
    "       c. Learning a correlated feature representation for keypoint detection and description? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Using SPyNet [5] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute camera motion from optical flow computed using SPyNet. We first uniformly sample points in an image, then consider the flow-displaced point in the other image as a match. A modified PyTorch implementation of SPyNet is provided in directory  $\\mathtt{Flow}$.\n",
    "1. Go to $\\mathtt{Flow}$ and run $\\mathtt{demo\\_spynet.py}$. \n",
    "2. Run the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runMatch\n",
    "dataset_path = '/datasets/cse152-252-sp20-public/dataset_SfM'\n",
    "feature_dir = 'Flow'\n",
    "runMatch.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Does SPyNet yield higher accuracy than the original $\\mathtt{libviso2}$? If so, why? If not, what steps can you take to improve?1. Does SPyNet yield higher accuracy than the original $\\mathtt{libviso2}$? If so, why? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain how SPyNet achieves accurate flow with significantly lower computational cost. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Using Sfm-learner for SFM\n",
    "Now we will use deep neural networks for SFM. Please follow the open-sourced Sfmlearner repository (https://github.com/ClementPinard/SfmLearner-Pytorch) and the paper (https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf) to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can you describe how photometric loss is implemented in Sfmlearner? Please Refer to the paper and the code. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can you train the model and show the training curve, validation curve, and visualization of predicted depth and warped images? Which loss is decreasing and which is increasing? Why? **(10 points)**\n",
    "\n",
    " Visualize the specific images: `val_Depth_Output_Normalized/0`, `val_Warped_Outputs/0` from tensorboard.\n",
    " \n",
    " Training data: \n",
    "`/datasets/cse152-252-sp20-public/sfmlearner_h128w416`.\n",
    "\n",
    "#### Install the environment\n",
    "`pip install -r requirements.txt`\n",
    "- fix scipy version problem: \n",
    "\n",
    "`pip install scipy==1.1.0 --user`\n",
    "\n",
    "#### Training script\n",
    "`cd SfmLearner-Pytorch`\n",
    "\n",
    "`python3 train.py /datasets/cse152-252-sp20-public/sfmlearner_h128w416 -b4 -m0.2 -s0.1 --epoch-size 3000 --sequence-length 3 --log-output`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When training the model, we use 3 consecutive frames. Can we construct the photometric consistency between 1st and 3rd frame? To be more specific, we can get the pose $T_{1,3} = T_{2,3} @ T_{1,2}$, where $T_{1,2}, T_{2,3}$ have already computed in the original code. Please add the constraint to the total loss. Show the results as in (2). **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate your models from (2), (3) on KITTI odometry dataset. What are the odometry metrics used in Sfm-learner. Please report the `ATE` and `RE` for sequence `09` and `10`. **(5 points)**\n",
    "\n",
    "  Data: `/datasets/cse152-252-sp20-public/kitti`.\n",
    "\n",
    "#### Evaluation script\n",
    "`python3 test_pose.py /path/to/posenet --dataset-dir /datasets/cse152-252-sp20-public/kitti --sequences 09`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- scp data to local machines:\n",
    "\n",
    "`scp -r <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cse152-252-sp20-public/sfmlearner_h128w416.zip`\n",
    "\n",
    "`scp -r <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cse152-252-sp20-public/kitti.zip`\n",
    "...\n",
    "- tensorboard: open jupyter notebook from the link after `launch-scipy-ml-gpu.sh`. Click new `Tensorboard ..`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 224–236, 2018.\n",
    "2. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n",
    "3. Andreas Geiger, Julius Ziegler, and Christoph Stiller. Stereoscan: Dense 3d reconstruction in real-time. In Intelligent Vehicles Symposium (IV), 2011.\n",
    "4. David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.\n",
    "5. Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In\n",
    "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4161–4170, 2017.\n",
    "6. A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms. http://www.vlfeat.org/, 2008.\n",
    "7. Lucas, Bruce D., and Takeo Kanade. \"An iterative image registration technique with an application to stereo vision.\" (1981): 674."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "py36-torch",
      "language": "python",
      "name": "myenv"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
